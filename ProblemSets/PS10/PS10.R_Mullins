###### PROBLEM SET 10 ######

### Install Packages if Needed
if(!require(rpart)) install.packages("rpart")
if(!require(e1071)) install.packages("e1071")
if(!require(kknn)) install.packages("kknn")
if(!require(nnet)) install.packages("nnet")
if(!require(mlr)) install.packages("mlr")
if(!require(class)) install.packages("class")

### Set Seed for Consistency 
set.seed(100)


### Read in data on incomes and add names to each column
income = read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data")
names(income) <- c("age","workclass","fnlwgt","education","education.num","marital.status","occupation","relationship","race","sex","capital.gain","capital.loss","hours","native.country","high.earner")
print(income)

### Description of Each Variable Below:


# From UC Irvine's website (http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)
#   age: continuous.
#   workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.
#   fnlwgt: continuous.
#   education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
#   education-num: continuous.
#   marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
#   occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.
#   relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
#   race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.
#   sex: Female, Male.
#   capital-gain: continuous.
#   capital-loss: continuous.
#   hours-per-week: continuous.
#   native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.


######################
# Clean up the data
######################


# Drop unnecessary columns
income$native.country <- NULL
income$fnlwgt         <- NULL


# Make sure continuous variables are coded as such
income$age            <- as.numeric(income$age)
income$hours          <- as.numeric(income$hours)
income$education.num  <- as.numeric(income$education.num)
income$capital.gain   <- as.numeric(income$capital.gain)
income$capital.loss   <- as.numeric(income$capital.loss)


# Combine levels of categorical variables that currently have too many levels
levels(income$education) <- list(Advanced = c("Masters,","Doctorate,","Prof-school,"), Bachelors = c("Bachelors,"), "Some-college" = c("Some-college,","Assoc-acdm,","Assoc-voc,"), "HS-grad" = c("HS-grad,","12th,"), "HS-drop" = c("11th,","9th,","7th-8th,","1st-4th,","10th,","5th-6th,","Preschool,"))
levels(income$marital.status) <- list(Married = c("Married-civ-spouse,","Married-spouse-absent,","Married-AF-spouse,"), Divorced = c("Divorced,","Separated,"), Widowed = c("Widowed,"), "Never-married" = c("Never-married,"))
levels(income$race) <- list(White = c("White,"), Black = c("Black,"), Asian = c("Asian-Pac-Islander,"), Other = c("Other,","Amer-Indian-Eskimo,"))
levels(income$workclass) <- list(Private = c("Private,"), "Self-emp" = c("Self-emp-not-inc,","Self-emp-inc,"), Gov = c("Federal-gov,","Local-gov,","State-gov,"), Other = c("Without-pay,","Never-worked,","?,"))
levels(income$occupation) <- list("Blue-collar" = c("?,","Craft-repair,","Farming-fishing,","Handlers-cleaners,","Machine-op-inspct,","Transport-moving,"), "White-collar" = c("Adm-clerical,","Exec-managerial,","Prof-specialty,","Sales,","Tech-support,"), Services = c("Armed-Forces,","Other-service,","Priv-house-serv,","Protective-serv,"))

# Break up the data:
n <- nrow(income)
train <- sample(n, size = .8*n)
test  <- setdiff(1:n, train)
income.train <- income[train,]
income.test  <- income[test, ]




####################
# TASK 5: INITIALIZE LEARNERS
####################

### Classification Test Setup
taskHighEarner = makeClassifTask(data = income.train, target = "high.earner")

### Cross Validation (3 Fold)
resample = makeResampleDesc(method = "CV", iters = 3)
#print(resample)

### Tuning Strategy (10 Guesses)
tune = makeTuneControlRandom(maxit = 10L)
#print(tune)

### Initialize Each of the 6 Learner Algorithms
     
 #### Note -  Included "response" as additional agruement ####

trees = makeLearner("classif.rpart", predict.type = "response")
logit = makeLearner("classif.glmnet", predict.type = "response")
neural = makeLearner("classif.nnet", predict.type = "response")
bayes = makeLearner("classif.naiveBayes", predict.type = "response")
kNN = makeLearner("classif.kknn", predict.type = "response")
SVM = makeLearner("classif.svm", predict.type = "response")




####################
# TASK 6: SET PARAMETERS
####################


### Set up hyperparameters of each algorithm


### 1. Tree Alg. Set Up 

treeParam = makeParamSet(
  makeIntegerParam("minsplit",lower = 10, upper = 50),
  makeIntegerParam("minbucket", lower = 5, upper = 50),
  makeNumericParam("cp", lower = 0.001, upper = 0.2))


### 2. Logit Alg. Set Up

logitParam =  makeParamSet(
  makeNumericParam("lambda",lower=0,upper=3)
  ,makeNumericParam("alpha",lower=0,upper=1))


### 3. Nueral Alg. Set Up

neuralParam = makeParamSet(
  makeIntegerParam("size", lower = 1, upper = 10),
  makeNumericParam("decay", lower = 0.1, upper = 0.5),
  makeIntegerParam("maxit", lower = 1000, upper = 1000))


### 4. Bayes Alg Note: Do not to crossvalidate/parameterize given the nature of the prior in Bayesian Methods


### 5. kNN Alg. Set Up

kNNParam = makeParamSet(makeIntegerParam("k", lower = 1, upper = 30))


### 6. SVM Alg. Set Up

SVMParam = makeParamSet(makeDiscreteParam("cost", values = 2^c(-2, -1, 0, 1, 2, 10)), 
  makeDiscreteParam("gamma", values = 2^c(-2, -1, 0, 1, 2, 10)))



  
####################
# TASK 7: TUNE MODELS
####################
  
  
### 1. Tune Tree Model 
  
treeTune = tuneParams(learner = trees,
                           task = taskHighEarner,
                           resampling = resample,
                           measures = list(f1, gmean),
                           par.set = treeParam,
                           control = tune,
                           show.info = TRUE)
### 2. Tune Logit Model
  
logitTune = tuneParams(learner = logit,
                           task = taskHighEarner,
                           resampling = resample,
                           measures = list(f1, gmean),      
                           par.set = logitParam,
                           control = tune,
                           show.info = TRUE)
  
### 3. Tune Nueral Model 
  
neuralTune = tuneParams(learner = neural,
                           task = taskHighEarner,
                           resampling = resample,
                           measures = list(f1, gmean),      
                           par.set = neuralParam,
                           control = tune,
                           show.info = TRUE)

### 4. Not Tuning Bayesian Model 
 

### 5. Tune kNN Model
  
kNNTune = tuneParams(learner = kNN,
                           task = taskHighEarner,
                           resampling = resample,
                           measures = list(f1, gmean),      
                           par.set = kNNParam,
                           control = tune,
                           show.info = TRUE)
  
### 6. Tune SVM Model
  
SVMTune = tuneParams(learner = SVM,
                           task = taskHighEarner,
                           resampling = resample,
                           measures = list(f1, gmean),      
                           par.set = SVMParam,
                           control = tune,
                           show.info = TRUE)




####################
# TASK 8: TRAIN MODELS
####################


### Apply Parameters to Each Learner

treePred = setHyperPars(learner=trees, par.vals = treeTune$x)
logitPred = setHyperPars(learner=logit, par.vals = logitTune$x)
neuralPred = setHyperPars(learner=neural, par.vals = neuralTune$x)
kNNPred = setHyperPars(learner=kNN, par.vals = kNNTune$x)
SVMPred = setHyperPars(learner=SVM, par.vals = SVMTune$x)

### Test Using Cross Validation 

sampleTree = resample(learner = treePred, task = taskHighEarner, resampling = resample, measures=list(gmean))
sampleLogit = resample(learner = logitPred, task = taskHighEarner, resampling = resample, measures=list(gmean))
sampleNeural = resample(learner = neuralPred, task = taskHighEarner, resampling = resample, measures=list(gmean))
samplekNN = resample(learner = kNNPred, task = taskHighEarner, resampling = resample, measures=list(gmean))
sampleSVM = resample(learner = SVMPred, task = taskHighEarner, resampling = resample, measures=list(gmean))

### Train Each Learner Model Using Training Data 

treeFinal =  train(learner = treePred, task = taskHighEarner)
logitFinal = train(learner = logitPred, task = taskHighEarner)
neuralFinal =  train(learner = neuralPred, task = taskHighEarner)
kNNFinal = train(learner = kNNPred, task = taskHighEarner)
SVMFinal =  train(learner = SVMPred, task = taskHighEarner)
bayesFinal = train(learner = bayes, task = taskHighEarner)

### Predict, In-Sample, for Each Learner

treeTest  = predict(treeFinal, newdata = income.test)
logitTest = predict(logitFinal, newdata = income.test)
neuralTest = predict(neuralFinal, newdata = income.test)
kNNTest = predict(kNNFinal, newdata = income.test)
SVMTest = predict(SVMFinal, newdata = income.test)
bayesTest = predict(bayesFinal, newdata = income.train)

### F1 and G-Mean for Each Learner

performance(treeTest, measures = list(f1, gmean))
performance(logitTest, measures = list(f1, gmean))
performance(neuralTest, measures = list(f1, gmean))
performance(bayesTest , measures = list(f1, gmean))
performance(kNNTest, measures = list(f1, gmean))
performance(SVMTest, measures = list(f1, gmean))
